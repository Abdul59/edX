# Introduction to whole genome sequencing

## Background [VIDEO]

This session provides a basic introduction to conducting a re-sequencing analysis using command-line tools to identify [single nucleotide polymorphims](http://ghr.nlm.nih.gov/handbook/genomicresearch/snp) (SNPs) and small insertion or deletions (InDels). We will be retracing all of the steps required to get from an Illumina FASTQ sequence file received from a sequencing facility as part of a genome sequence analysis all the way to germline variant calls and variant prioritization. As part of this module we will also discuss approaches to test for changes in structural variation, explore differences between germline and somatic variant calling, and provide an overview on how to best scale the approach when handling much larger sample sets. 

To keep things manageable and allow algorithms to finish within a few minutes we picked a single sample from the 1000 Genomes Project: NA12878, sequenced as part of a CEU trio which has become the de-facto standard to benchmark variant calling approaches and is used by groups  such as the [Genome in a Bottle consortium](http://www.nist.gov/mml/bbd/ppgenomeinabottle2.cfm) to better understand the accuracy of sequencing technologies and bioinformatic workflows.

## Introduction to sequencing

### Introduction to sequencing technologies [SLIDES]

* Use own materials and slides from https://bioshare.bioinformatics.ucdavis.edu/bioshare/download/00000dyb3riyzvb/Next%20Gen%20Fundamentals%20Boot%20Camp%20-%202013%20June%2018.pdf and http://training.bioinformatics.ucdavis.edu/docs/2014/09/september-2014-workshop/_downloads/Monday_JF_HTS_lecture.pdf


### Introduction to resequencing analysis [SLIDES]

* Gene panels, exomes or whole genomes?
* Individuals, families or cohorts?
* High level summary of a best practice workflow


## Introduction to the command line [VIDEO]

* Options: Galaxy/Cloudman (point at our course). Command line. 
* We want to scale, hence command line which means learning basic Unix

### Requirements [SCREENCAST]

* Setting up VirtualBox
* Downloading VM
* Testing that it works

### Introduction to Unix [SCREENCAST]

* Introduction and the terminal
* Files and directories
* Handling files
* Redirections and permissions

**Resources:** 

* http://www.ee.surrey.ac.uk/Teaching/Unix/
* http://korflab.ucdavis.edu/unix_and_Perl/
* http://software-carpentry.org/v5/novice/shell/index.html 


## Resequencing workflow [VIDEO]

* Quick recap of the aims for this module

### Obtaining read data [SCREENCAST/SLIDES]

To call variants we need an sequence data in [FASTQ format](https://dl.dropbox.com/u/407047/Blog/Documents/Literature/QC/Nucleic%20Acids%20Res%202009%20Cock.pdf) (PDF). We also need a [reference genome](https://en.wikipedia.org/wiki/Reference_genome) in [FASTA format](https://en.wikipedia.org/wiki/FASTA_format). As mentioned before, the CEU hapmap sample `NA12878` is a good starting point as it is widely used to asses sequencing quality and workflow accuracy. It is also publicly available and comes with a 'truth' set that is constantly being improved by the [Genome in a Bottle consortium](http://www.genomeinabottle.org/). Sequencing data is available from a number of sources including [Illumina's Platinum Genome collection](http://www.illumina.com/platinumgenomes/), but for this course we will be using a low-coverage sequence data set (at ~5X genomic coverage) generated by the [1000 Genomes Project](http://www.1000genomes.org/). Specifically, we will be using reads just from chromosome 20 -- i.e., these reads have previously been aligned to the human genome and will be already of reasonably good quality, something you cannot expect for regular sample data.

> Grab data from 1000G at ftp://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/data/NA12878/alignment/NA12878.chrom20.ILLUMINA.bwa.CEU.low_coverage.20121211.bam and convert back to FASTQ data. Check whether this is paired end or not. Assuming it is, `samtools sort -n aln.bam aln.qsort` followed by `bedtools bamtofastq -i aln.qsort.bam -fq aln.end1.fq -fq2 aln.end2.fq`

> Mix in bad quality data (Shannan, Rory)

We will also require a reference genome for the alignment. We could align to the whole human genome, but since we are focusing on reads from chromosome 20 we will just get a copy of this chromosome:

> Need to move to a different location. GitHub?

	wget http://bioinformatics.bc.edu/marthlab/download/gkno-cshl-2013/chr20.fa

Take a look at your reference chromosome using `less` or `head`. If this is the reference genome, why are you seeing so many `N` nucleotides in the sequence?  


### Quality Controls [VIDEO]

Reads from the FASTQ file need to be mapped to a reference genome in order to identify systematic differences between the sequenced sample and the human reference genome. However, before we can delve into read mapping, we first need to make sure that our preliminary data is of sufficiently high quality. This involves several steps:

1. Obtaining summary quality statistics for the reads and reviewing diagnostic graphs 
2. Filtering out genetic contaminants (primers, vectors, adaptors)
3. Trimming or filtering low-quality reads
4. Recalculating quality statistics and review diagnostic plots on filtered data

This tends to be an interative process, and you will have to make your own decisions on what you consider acceptable quality. For the most part sequencing data tends to be good enough that it won’t need any filtering or trimming as modern aligners will _soft-clip_ reads that do not perfectly align to the reference genome — we will show you examples of this during a later module. 

> Use own slides and materials from http://training.bioinformatics.ucdavis.edu/docs/2014/09/september-2014-workshop/_downloads/Monday_JF_QAI_lecture.pdf


#### Exploring the FASTQ file [SCREENCAST/SLIDES]

Go back to where you downloaded the sequencing data in FASTQ format and take a look at the contents of your file:

> Navigate around, head/less data, check size

The reads are just for human chromosome 20 which amounts to read data from about 2% of the human genome, and sequenced at only 5X coverage. Current standard for whole genome sequencing is closer to 30-40X, so you can expect _actual_ WGS read data files to be about 400 times larger than your test data.
 
> Walk them through FASTQ format. What additional information does FASTQ contain over FASTA? Pick the first sequence in your FASTQ file and note the identifier. What is the quality score of it's first and last nucleotide, respectively?

For a quick assessment of read quality you will want to stick to standard tools such as [FASTQC](http://www.bioinformatics.bbsrc.ac.uk/projects/fastqc/) in most cases. FASTQC will generate a report in HTML format which you can open with any web browser. 

	fastqc --version  # record the version number!
	fastqc -o ../scratch/FastQC-r1-raw -f fastq --contaminants share/apps/FastQC/Configuration/contaminant_list.txt ../data/reads1.fastq 
	Repeat with 2nd read pair

In order to look at the FastQC output you will need to copy the html report file outside of the VirtualBox environment and look at in a browser.

	> Need to figure out how to get data in/out of the VM. Walk them through the FASTQC report as in the regular course. Overall quality, number of reads. Differences in paired read (1 vs 2 overall quality)

> Walk them through the [Core conference call plots](http://bioinfo-core.org/index.php/9th_Discussion-28_October_2010) 


#### Error sources in sequencing [SLIDES]

* http://training.bioinformatics.ucdavis.edu/docs/2014/09/september-2014-workshop/_downloads/Monday_JF_QAI_lecture.pdf and our own course materials. Where do sequencing errors come from, connect to the initial slides on sequencing technologies.


#### Screen for adapter sequences [SCREENCAST/SLIDES]

Depending on the source of your data and the FASTQC report you might want to trim adapter sequences from your reads with a tool such as [cutadapt](http://cutadapt.readthedocs.org/en/latest/guide.html).

> If we want to do this we will need to mix in reads with adapter sequences. Could use help with generating that data. Show http://cutadapt.readthedocs.org/en/latest/_images/adapters.svg

For our data set we will trim off a standard adapter from the 3'-ends of our reads. Cutadapt can handle paired-end reads in one pass (see the documentation for details). While you can run cutadapt on the paired reads separately it is highly recommended to process both files at the same time so cutadapt can check for problems in the input data. The `-p` / `--paired-output` flag ensures that cutadapt checks for proper pairing of your data and will raise an error if read names in the files do not match. It also ensures that the paired files remain synchronized if the filtering and trimming ends up in one read being discarded -- cutadapt will remove the matching paired read from the second file, too.

	cutadapt -a ADAPTER_FWD -o out.1.fastq -p out.2.fastq reads.1.fastq reads.2.fastq

cutadapt is very flexible and can handle multiple adapters in one pass (in the same or different position), take a look at the excellent documentation. It is even possible to trim more than one adapter from a given read though at this point you might want to go back to the experimental design.

> Redirect statistics to a logfile. Bring up the need to keep track of this information somehow. Logbook, written notes, along with the version of the tool used. Show how to check for a version.

As you are working through your data manually on the command line it is important that you keep track of what you did. A straightforward approach to do this is to copy and paste all commands that you typed into the terminal into a seperate text document. Make sure to also keep track of _where_ you are in your Unix enviromnent (the absolute directory path). For all tools you use you should also keep track of the _version_ as updates to your software might change the results. Most, if not all, software can be tested for their version with the `-v` or `--version` command switch:

	cutadapt -v

Keep track of where the output ends up if not mentioned explicitly in the command itself. It can also be helpful to keep track of any output your software generates in addition to the final result. 

	> Check what cutadapt reports, add redirect: 2> foo.stdErr

This will allow you to go back to your recorded logfiles to explore additional information, e.g., how many adapters were removed. The `2>` part works just like a standard rediction pipe, but redirects _standard error_ messages only (the default `>` redirection is equivalent to writing `1>` which stands for _standard out_). Different tools have different ways of reporting log messages and you might have to experiment a bit to figure out what output to capture.

> Walk them through the cutadapt output, http://cutadapt.readthedocs.org/en/latest/guide.html#cutadapt-s-output


### Trimming and filtering by quality [SCREENCAST]

After reviewing the quality diagnostics from FASTQC decide on whether you want to trim lower quality parts of your read and filter reads that have overall lower quality. This step is optional in that aligners and subsequent variant calling steps can be configured to discard reads aligning poorly or to multiple genomic locations, but it can be good practice. For read data sets of particularly poor quality a trimming and filtering step will also reduce the overall number of reads to be aligned, saving both processing time and storage space. 

There are a number of tools out there that specialize in read trimming, but luckily cutadapt can _also_ handle quality-based read trimming with the `-q` (or `--trim-qualities`) parameter. This can be done before or after adapter removal, and expects your FASTQ data to be in standard Sanger FASTQ format. Cutadapt uses the same approach as aligners such as bwa: it removes all bases starting from the _end_ of the read where the quality is smaller than a provided threshold while allowing some good-quality bases among the bad quality ones. 

> Can walk them through the algorithm if we want to use the opportunity to explain it?
> Subtract the given cutoff from all qualities; compute partial sums from all indices to the end of the sequence; cut sequence at the index at which the sum is minimal. The basic idea is to remove all bases starting from the end of the read whose quality is smaller than the given threshold. This is refined a bit by allowing some good-quality bases among the bad-quality ones. Assume you use a threshold of 10 and have these quality values:
> 42, 40, 26, 27, 8, 7, 11, 4, 2, 3
> Subtracting the threshold gives:
> 32, 30, 16, 17, -2, -3, 1, -6, -8, -7
> Then sum up the numbers, starting from the end (partial sums). Stop early if the sum is greater than zero:
> (70), (38), 8, -8, -25, -23, -20, -21, -15, -7
> The numbers in parentheses are not computed (because 8 is greater than zero), but shown here for completeness. The position of the minimum (-25) is used as the trimming position. Therefore, the read is trimmed to the first four bases, which have quality values 41, 40, 25, 27.


	cutadapt -q 20 -o out1.fastq -p out2.fastq input1.fastq input2.fastq

After you have filtered your data generate another `FASTQC` quality report and compare the report to your previous one.

> Do you have an improvement in read quality? What has changed and why?
> Quick detour on what parameters to use. From the UC Davis notes: "Keep in mind that you may want to trim and/or error correct differently, depending on what you’re doing downstream. For example, most aligners are tolerant of a few N’s in the sequence, but some assemblers will do weird things with N’s, and if you have enough depth, it’s better to toss reads containing N’s. There’s lots of room for experimentation and optimization. Take chances, make mistakes!"

Move or copy the final FASTQ files do a separate directory for the next stage, read alignment. Keep track of where you move files at all times.


### Read alignment [SLIDES/SCREENCAST]

> For the whole section use our own slides in combation with slides from http://training.bioinformatics.ucdavis.edu/docs/2014/09/september-2014-workshop/_downloads/Tuesday_JF_Alignment_lecture.pdf and https://www.broadinstitute.org/gatk/events/2038/GATKwh0-BP-1-Map_and_Dedup.pdf

Next we are going to look at the steps we need to take once we have a clean, filtered FASTQ file that is ready for alignment. This alignment process consists of choosing an appropriate reference genome to map your reads against, and performing the read alignment using one of several alignment tools such as [NovoAlign](TBA) or [BWA-mem](TBA). The resulting output file can then be checked again for the quality of alignment as all subsequent steps rely on the reads having been placed at the correct location of the reference genome, so it pays to spend some extra time to verify the alignment quality.

To avoid excessive runtimes during later steps we are not aligning the sequences against the whole human genome, but will just use chromosome 20 of human genome build 19 (hg19) which you downloaded before.  

> Quick intro to genome builds. We are using a reduced reference here

> Discuss why it is normally a bad idea to just align to a subset: reads will be forced to align to the reference provided even though there might be much better fits for a given read in other parts of the genome, leading to errors. This is why you also want to use all alternative haplotype assemblies as part of your reference genome. Even if you are not planning to _use_ them during variant calling they are useful as bait, ensuring reads end up where they fit best.

For our example we will be using [bwa-mem](https://github.com/lh3/bwa) which has become one of the standard tools for aligning Illumina reads >75bp to large genomes.

> Aligners vs assembly. Aligner choices. Concepts. 

> Have them look at the bwa options page, http://bio-bwa.sourceforge.net/bwa.shtml. Tweaking parameters requires a good benchmark set (simulated or otherwise) to assess impact

For the actual alignment we will need our chr20 reference sequence and the trimmed read data in FASTQ format. Create a new directory and copy or move the relevant files over:

> cmd line

Before we can align reads, we must index the genome sequence:

	bwa index ../data/fieX174.fa

With the index in place we can align the reads using BWA's _Maximal Exact Match_ algorithm (bwa-mem, see the [manual](http://bio-bwa.sourceforge.net/bwa.shtml) for bwa options). 

> Confirm that BWA creates SAM output

	bwa  # version?
	bwa mem -M ../data/fieX174.fa 
     ../scratch/r1.atqt.fq ../scratch/r2.atqt.fq 2> \
     ../scratch/mem.stdErr

> Take a look at the output file. Note it’s size relative to FASTQ. How long did it take to run? Now extrapolate to how long you would expect this tool to run when mapping to the entire genome (approximately). What about using whole genome data instead of whole exome?

#### SAM/BAM [SLIDES/SCREENCAST]

> Again use own slides and material from http://training.bioinformatics.ucdavis.edu/docs/2014/09/september-2014-workshop/_downloads/Tuesday_JF_Alignment_lecture.pdf to explain BAM/SAM

Many aligners produce output in ["SAM" format](http://samtools.sourceforge.net/) (Sequence Alignment/Map format, see [publication from Heng Li](https://dl.dropbox.com/u/407047/Blog/Documents/Literature/Exome%20Seq/Bioinformatics%202009%20Li-3.pdf) (PDF) for more details). 

	less reads.paired.vs.canFam2.chr1.sam

> Check with Brad. Do I need to re-header to add read group/RG and sample ID/SM information to the SAM or BAM file, or does FreeBayes not require this? If so:
	> "Before we go into GATK, there is some information that needs to be added to the BAM file, using “AddOrReplaceReadGroups”. To your marked duplicates BAM file, we will add samp1 as “Read Group ID”, “Read Group sample name” and “Read group library”. “Read group platform” has to be illumina as the sequencing was done using an Illumina instrument. “Read group platform unit” we are going to set as run:"
> bwa sampe -r "@RG\tID:samp1\tSM:samp1" -f reads.paired.vs.canFam2.chr1.sam \ ./canFam2.chr1.fa reads1.sai reads2.sai \
	  reads1.scythe.sickle.fastq reads2.scythe.sickle.fastq

> Explore the SAM format. What key information is contained? What is in the header?

#### Mark duplicates [SLIDES/SCREENCAST]

> Discuss order with Brad. samblaster takes SAM input and does not need it sorted, if I recall correctly. FreeBayes _does_ need it sorted.

We now have paired end reads aligned to chromosome 20 and are almost ready for variant calling, but we need to remove duplicate reads first. These originate mostly from library preparation methods (unless sequenced very deeply) and bias subsequent variant calling.

> Material from https://www.broadinstitute.org/gatk/events/2038/GATKwh0-BP-1-Map_and_Dedup.pdf

We will be using [samblaster](https://github.com/GregoryFaust/samblaster) for this step.

	samblaster

From samblaster's description: _"samblaster is a fast and flexible program for marking duplicates in read-id grouped paired-end SAM files. It can also optionally output discordant read pairs and/or split read mappings to separate SAM files [..]". This latter feature comes in handy when looking for structural variation, but here we are mostly interested in its ability to flag duplicate reads. It expected paired end data with a sequence header and grouped by read-id, that is, all reads with the same read-id are in adjacent lines -- the default output for most aligners sich as bwa-mem. samblaster can either discard duplicates or (the preferred option) simply mark them with a specific flag in the SAM file. 

In order to be called a 'duplicate' reads need to match on the sequence name, strand, and where the 5' end of the read would end up on the genomic sequence if the read is fully aligned, i.e., when ignoring any clipped reads. 

> Check. Does our use of cutadapt mess with the duplicate filter? Tempted to rely on soft-clipping and just skip that step during cutadapt.

Lastly, to speed up processing we will need to use SAMtools to convert the SAM file to a BAM file, allowing subsequent methods and viewers to navigate the data more easily.

	samtools  # version?
	/share/apps/samtools/samtools view -uhS \
     	../scratch/mem-alignments.sam.gz | \
     	/share/apps/samtools/samtools sort - \
     	../results/mem-alignments

As a sidenote, samblaster and many other tools can read from standard input and write to standard out and can thus be easily inserted into a very simple 'pipeline'. For example:

	bwa mem index samp.r1.fq samp.r2.fq | samblaster | samtools view -Sb - > samp.out.bam

This runs the bwa-mem alignment, pipes the resulting file directly into samblaster which passes the results on to samtools for conversion into a BAM file.

As final preparation steps we sort the BAM file by genomic position, something that SAMtools' `sort` subcommand handles. It asks for a _prefix_ for the final name and appends '.bam' to it:

	samtools sort reads.paired.vs.canFam2.chr1.bam reads.paired.vs.canFam2.chr1.bam.sorted

To speed up access to the BAM file (and as a requirement of downstream tools) we index the BAM file, once again using samtools:

	samtools index reads.vs.canFam2.chr1.rg.bam


#### Assess the alignment [SCREENCAST]

> Do we want to use Chanjo here? Might be over the top. https://github.com/robinandeer/chanjo and http://kerncomputing.blogspot.co.uk/2014/07/coverage-case-study-in-extending-bcbio.html

Let's take a look at how bwa ended up aligning our reads to the reference chromosome using the the Broad’s Integrated Genome Viewer, IGV. You can access IGV via the [Broad's website](http://www.broadinstitute.org/software/igv/log-in) using the 'Java Web Start' button, but you will have to register first. Start IGV which will take a while to load and may ask for permissions; as it is a Java application you may also be asked to install Java first.

> Need a section on a) how to get data outside of the VM, and b) how to initialize IGV. Walk them through adding the BAM file to the viewer, indexing the chr20 file and adding it, and browsing around. Find regions with no/very high coverage, zoom in on these (coordinates or gene based). 

In the meantime, prepare the data for viewing. You will need the alignment in BAM format, the corresponding index file (*.bai) and the chromosome 20 reference sequence which you will also have to index:

	samtools faidx XXX

Export these three files from your VM to your desktop and import them into IGV, starting with the reference chromosome ('Genomes', 'Load Genome from file') and followed by the alignment ('File', 'Load from file'). 

> Show some basic settings. Expanded / collapsed views. Color alignment by. Ritght click on track for on the fly adjustments.

> How uniform is the coverage for this region? Can you spot regions where there are variants (red lines)? Can we find regions with Indels that are messy?


### Calling Variants [SLIDES/SCREENCAST]

We have our sequence data, we cleaned up the reads, aligned them to the genome, sorted the whole thing and flagged duplicates. Which is to say we are now finally ready to find sequence variants, i.e., regions where the sequenced sample differs from the human reference genome. 

> Basic background information can be taken from the GATK-based talks from Jie Li, e.g., http://training.bioinformatics.ucdavis.edu/docs/2014/09/september-2014-workshop/_downloads/GATKvariantdiscovery_091114.pdf (such as the introduction to variant types). We can also recycle some of our own slides from the Galaxy course.

> FreeBayes-specific background can be based on Eric's talk, http://clavius.bc.edu/~erik/CSHL-advanced-sequencing/CSHL%20advanced%20sequencing%20variant%20detection.pdf. Some of the materials can also be used in the introduction to re-sequencing analysis section. In particular relevant to highlight the basic approach, why haplotypes help (additional information again, similar to how we can use information from related samples), how it has an impact on functional assessment to call variants while being haplotype aware.

Some of the more popular tools for calling variants include samtools, the [GATK suite](https://www.broadinstitute.org/gatk/guide/best-practices?bpm=DNAseq) and FreeBayes. While it can be useful to work through the GATK Best Practices we will be using [FreeBayes](https://github.com/ekg/freebayes) in this module as it is just as sensitive and precise, but has no license restrictions. 

FreeBayes uses a Bayesian approach to identify SNPs, InDels and more complex events as long as they are shorter than individual reads. It is haplotype based, that is, it calls variants based on the reads aligned to a given genomic region, not on a genomic position. In brief, it looks at read alignments from an individual (or a group of individuals) to find the most likely combination of genotypes at each reference position and produces a variant call file (VCF) which we will study in more detail later. It can also make use of priors (i.e., known variants in the populations) and adjust for known copy numbers. Like GATK it includes a re-alignment step that left-aligns InDels and minimizes alignment inconsistencies between individual reads. 

> Can mention any of this, but it's nitpicking: "The need for base quality recalibration is avoided through the direct detection of haplotypes. Sequencing platform errors tend to cluster (e.g. at the ends of reads), and generate unique, non-repeating haplotypes at a given locus. Variant quality recalibration is avoided by incorporating a number of metrics, such as read placement bias and allele balance, directly into the Bayesian model. (Our upcoming publication will discuss this in more detail.)"

> Switch to command line. Tutorial itself at http://clavius.bc.edu/~erik/CSHL-advanced-sequencing/freebayes-tutorial.html

> Requirements prior to this step:
> * Align your reads to a suitable reference (e.g. with bwa or MOSAIK)
> * Ensure your alignments have read groups attached so their sample may be identified by freebayes. Aligners allow you to do this, but you can also use bamaddrg to do so post-alignment.
> * Sort the alignments (e.g. bamtools sort).

In principle FreeBayes only needs a reference in FASTA format and the BAM-formatted alignment file with reads sorted by positions and with read-groups attached. To see _all_ options take a look at the manual or run:

	freebayes --help

For now, we will simply call variants with the default parameters which should only take a couple of minutes for our small data set:

	freebayes -f chr20.fa NA12878.chrom20.ILLUMINA.bwa.CEU.low_coverage.20121211.bam  NA12878.chr20.freebayes.vcf

> Remember the 400x size reduction. Assess how long this might have taken for all of h19

Like other methods we looked at FreeBayes follows standard Unix conventions and can be linked to other tools with pipes. FreeBayes allows you to only look at specific parts of the genome (with the `--region` parameter), to look at multiple samples jointly (by passing in multuple BAM files which are processed in parallel), consider different ploidies or recall at specific sites. The GitHub page has many examples worth exploring. 


#### Understanding VCFs [SLIDES/SCREENCAST]

The output from FreeBayes (and other variant callers) is a VCF file (in standard 4.1 format), a matrix with variants as rows and particpants as columns. It can handle allelic variants for a whole population of samples or simply describe a single sample as in our case.

> Guide them through a standard VCF. Use http://training.bioinformatics.ucdavis.edu/docs/2014/09/september-2014-workshop/_downloads/Tuesday_JF_Alignment_lecture.pdf for VCF description, ideally in context of IGV. http://training.bioinformatics.ucdavis.edu/docs/2014/09/september-2014-workshop/_downloads/GATKvariantdiscovery_091114.pdf also works.

Now take a look at the results FreeBayes generated for the NA12878 data set:

	less -S NA12878.chr20.freebayes.vcf

You will see the header which describes the format, when the file was created, the FreeBayes version along with the command line parameters used and some additional column information:

~~~
##fileformat=VCFv4.1
##fileDate=20131121
##source=freeBayes v0.9.9.2-23-g8a98f11-dirty
##reference=chr20.fa
##phasing=none
##commandline="freebayes -f chr20.fa NA12878.chrom20.ILLUMINA.bwa.CEU.low_coverage.20121211.bam"
 ##INFO=<ID=NS,Number=1,Type=Integer,Description="Number of samples with data">
~~~


Followed by the variant information: the reference chromosome and position on the reference, an identifier (to be filled by annotation), the reference and called genotype along with a quality assessment and various information fields including the allele frequency:

~~~
#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO
20      61795   .       G       T       127.882 .       AB=0.454545;ABP=3.20771;AC=1;AF=0.5
20      63244   .       A       C       90.7756 .       AB=0;ABP=0;AC=2;AF=1
~~~

The `QUAL` field is an estimate of the probability that there really _is_ a polymorphism at this position of the genome. 


#### Filtering VCFs [SCREENCAST]

By default FreeBayes does almost no filtering, only removing low-confidence alignments or alleles supported by low-quality base quality positions in the reads. It  expects the user to subsequently flag or remove variants that have a low probability of being true. A method from the [vcflib package](https://github.com/ekg/vcflib), `vcffilter`, enables us to quickly subset our VCF based on the various quality attributes:

	freebayes -f ref.fa aln.bam | vcffilter -f "QUAL > 20" >results.vcf

This example removes any sites with an estimated probability of not being polymorphic less than 20 -- this is again the PHRED score described in our FASTQ survey, and matches a probablily of 0.01, or a probability of a polymorphism at this site >= 0.99. 

Another nifty tool from the same package, `vcfstats`, allows us to aggregate our VCF data for chromosome 20 and to compare the impact of different filtering steps:

	vcfstats NA12878.chr20.freebayes.vcf

> Walk them through the summary for this file; we can also provide the same stats for a 'full' NA12878 run if needed. The file will include information about small variants, such as the counts of SNPs, indels, and other variant types, the transition/transversion ratio, and a summary of the indel length-frequency spectrum.

For example, note the different ts/tv ratio (the transition/transversion rate of which tends to be around 2-2.1 for the human genome, although it changes between different genomic regions):

	vcffilter -f "QUAL > 20" NA12878.chr20.freebayes.vcf | vcfstats | grep "ts\|bial"

Again, we use pipes to first create a VCF subset that we then aggregate, finally using grep to pick out the statistic of interest. Compare this to the variants called with lower confidence:

	vcffilter -f "QUAL < 20" NA12878.chr20.freebayes.vcf | vcfstats | grep "ts\|bial"

Filters in the Q20-Q30 range have shown to work reasonably well for most samples, but might need to be combined with filters on user statistics such as regions with very low coverage, or perhaps surprisingly at first glance in regions of exceptionally high coverage -- these are frequenly repeat regions attracting many mismapped reads. Other good filters include those that test for strans biases, that is, excluding variants found almost exlusively only on one strand which indicates a read duplication problem, or those that test for allelic frequency which, at least for germline mutations, should be close to 50% or 100% given sufficient coverage.


#### Benchmarks [SCREENCAST]

When trying to figure out which possible disease-causing variants to focus on it becomes crucial to identify likely false positive calls (but also to get a general idea of the false negative rate). One way to get to these metrics is to run data for which 'gold standards' exist. As mentioned before, NA12878 is one of the best studied genomes and has become such a gold standard, in part through efforts by NIST's Genome in a Bottle consortium which has created and validated variant calls from a large number of different sequencing technologies and variant calling workflows. Let's download both their list of high confidence variance as well as a target list of genomic regions where comparisons are useful (e.g., it includes repetive regions which are error prone):

	wget ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/variant_calls/NIST/NISTIntegratedCalls_13datasets_130719_allcall_UGHapMerge_HetHomVarPASS_VQSRv2.17_all_nouncert_excludesimplerep_excludesegdups_excludedecoy_excludeRepSeqSTRs_noCNVs.vcf.gz

	wget ftp://ftp-trace.ncbi.nih.gov/giab/ftp/data/NA12878/variant_calls/NIST/union13callableMQonlymerged_addcert_nouncert_excludesimplerep_excludesegdups_excludedecoy_excludeRepSeqSTRs_noCNVs_v2.17.bed.gz
	gunzip *.vcf.gz
	gunzip *.bed.gz

Take a look at the `*.bed` file. It's just a set of genomic coordinates:

	less *.bed.gz

For the purpose of this lecture we also grab a curated set of the GiaB variants available from the Broad Institute:

	wget http://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/working/20130806_broad_na12878_truth_set/NA12878.wgs.broad_truth_set.20131119.snps_and_indels.genotypes.vcf.gz
	wget http://ftp.ncbi.nlm.nih.gov/1000genomes/ftp/technical/working/20130806_broad_na12878_truth_set/NA12878.wgs.broad_truth_set.20131119.snps_and_indels.genotypes.vcf.gz.tbi

The `tbi` file is an index created by tabix, another part of samtools handling tab-delimited genomic position files. We can use it to get just the chr20 subset from the Broad data:

	tabix -h NA12878.chr20.broad_truth_set.20131119.snps_and_indels.genotypes.vcf.gz 20

~~~
Or, you can download a pre-made set:

wget http://clavius.bc.edu/~erik/NA12878/NA12878.chr20.broad_truth_set.20131119.snps_and_indels.genotypes.vcf.gz
wget http://clavius.bc.edu/~erik/NA12878/NA12878.chr20.broad_truth_set.20131119.snps_and_indels.genotypes.vcf.gz.tbi
~~~

Next we want to remove any variants outside of the high-confidence intervals defined by GiaB. Again, pipes are our friend saving us from having to unzip the input data; we can stream the VCF file directly into vcfintersect, a part of vcflib:

	zcat NA12878.chr20.broad_truth_set.20131119.snps_and_indels.genotypes.vcf.gz | vcfintersect -b union13callableMQonlymerged_addcert_nouncert_excludesimplerep_excludesegdups_excludedecoy_excludeRepSeqSTRs_noCNVs_v2.17.bed > REDIRECT

Finally, we only want to keep variants marked as 'true positive' in the curated Broad version of the GiaB file:

    cat REDIRECT | grep "^#\|TRUE_POSITIVE" >NA12878.chr20.broad_truth_set.TP.callable_regions.vcf

We now need to repeat this for our own calls:

	cat NA12878.chr20.freebayes.vcf | vcfintersect -b union13callableMQonlymerged_addcert_nouncert_excludesimplerep_excludesegdups_excludedecoy_excludeRepSeqSTRs_noCNVs_v2.17.bed  NA12878.chr20.freebayes.callable_regions.vcf

Vcflib comes with a few additional tools to generate the plots from our calls and the truth set:

	vcfroc -r chr20.fa \
    -t NA12878.chr20.broad_truth_set.callable_regions.vcf \
    NA12878.chr20.freebayes.vcf >broad_truth_set.roc.tsv

We can generate a plot of this using some tools in vcflib:

> This is a little bit over the top for this tutorial. Just cut and paste, no need to understand? Wish there better/easier methods to get to this. Particularly since they then need to take this outside of the VM, into R, etc. Maybe just show the results?

	( vcfroc -r chr20.fa -t \
    NA12878.chr20.broad_truth_set.TP.callable_regions.vcf \
    NA12878.chr20.broad_truth_set.TP.callable_regions.vcf \
    | prependcol set answers; \
    cat broad_truth_set.roc.tsv \
    | prependcol set freebayes | grep -v set ) >roc.tsv
	plot_roc.r freebayesNA12878 answers roc.tsv 0 1 0 1

> Here, prependcol is a script that prepends a column named "set" with each row set to "answers" or "freebayes."

> The resulting image freebayesNA12878.both.png, shows that we can achieve < 5% FDR in this sample (assuming perfect sensitivity in the truth set--- which is unlikely), and that our overall sensitivity is rather low. This matches our expectations given the low coverage of the sample.

For now, simply filter your VCF file by 



#### Visually exploring the final VCF

Go back to IGV which should still be running (if not, revisit the Alignment Assessment module to start IGV and import both the indexed reference chromosome and your aligned reads in BAM format). 

> Need to index the VCF file.
> Need to figure out whether there is anything to see
> Need to understand how FreeBayes filters things, do they show up in IGV?

Check for good matches between variant calls and the reads. Try to find homozygous and heterozygous variants, and check for cases where the reads indicate a difference to the reference genome, but no variant was called.



#### Annotating a VCF file

During the next session we will annotate the VCF file and use this annotation to select a small number of high-confidence, novel variants that might be of interest.

During the this session you will be annotating the variants you identified with known information such as variant calls from the [HapMap project](http://hapmap.ncbi.nlm.nih.gov/) and filter out low-quality calls to improve the chance of reliably identifying functionally relevant variants. If you feel confident do annotate both VCF files and compare the results, alternatively just pick one of the two and limit your analysis to it.

You will be using a number of additional files that contain information about known variants (`dbSNP` and data from the `HapMap` project in VCF format). Data is in the `Reference` folder and was retrieved from the [GATK resource bundle](http://gatkforums.broadinstitute.org/discussion/1213/whats-in-the-resource-bundle-and-how-can-i-get-it). Pull in the dbSNP variant information (just the site information) from the reference library. If you are curious why we are using the 'before 129' version, a number of articles from [MassGenomics](http://massgenomics.org/2012/01/the-current-state-of-dbsnp.html) and FinchTalk [here](http://finchtalk.geospiza.com/2011/01/dbsnp-or-is-it.html) and [here](http://finchtalk.geospiza.com/2011/03/flavors-of-snps.html) provide lots of context; in brief, this excludes the majority of variants detected by next-gen sequencing, the majority of which have not have been properly annotated just yet.

* To make the processing easier filter for `chr22` only using the `Filter` tool from the Filter and Sort section
* Use the VCFTools `Annotate` function from VCFTools to merge your filtered chr22 dbSNP information with the GATK output. 

Check the new VCF file; the vast majority of variants now have an `rs` identifier in the `ID` column. 

> Look up one or two identifiers in the [dbSNP database](http://www.ncbi.nlm.nih.gov/projects/SNP/) and report on the kind of information that is available.







### Prioritizing variants

> I'd like to switch gears here and rip out the existing materials, replacing it with the Gemini tutorial, http://quinlanlab.org/tutorials/cshl2013/gemini.html. Drawback: another 2GB download, all in all. Still think it's worth it as we cannot filter the 5x NA12878 data in any meaningful way, and a disease gene hunt might be more interesting. This also saves me from having to figure out how to get snpEff and Co to run...


A vast number of algorithms exists to quantify the likely impact of a genetic variant. We will explore just a few options to generate additional information for single SNPs and provide pointers to approaches that try to identify functional enrichment from multiple samples and patient.

To simplify the analysis we will be using an aggregator, i.e., a tool that combines multiple annotation services, which accepts VCF-formatted data as input. Upload your annotated VCF file to [VAT](http://vat.gersteinlab.org/index.php) and explore the report. In the output, check for deleterious/damaging non-synonymous mutations or loss-of-function mutations. Pick one mutation and review it in IGV -- what is its coverage? 

> What kind of change is the variant causing?

VAT can also handle output from multiple samples (variant files), see the [VAT report for lanes 7/8 aligned with Bowtie](http://vat.gersteinlab.org/summary.php?dataSet=vat.222&setId=222&annotationSet=gencode7&type=coding) online.

If you are analyzing multiple samples command line based tools can be useful. One of the standard frameworks for this is [snpEff](http://snpeff.sourceforge.net/); a sample output file for lanes 7 and lane 8 is [available online](http://dynamic.gersteinlab.org/people/lh372/vat_cgi?mode=process&dataSet=vat.27560&annotationSet=gencode7&type=coding).


#### Finding novel variants

Assume we are only interested in 'novel' mutations. Repeat the generic filter, this time removing anything with an `rs` identifier (a known dbSNP mutation) in the `REF` column (`column 3`). 

> How many novel variants are there? Write down one or two positions for later.

As before, send your BAM file and the VCF file to IGV (suggested: the annotated VCF file with the 'PASS' information included).

> Explore one or two of the novel variants you wrote down from your manually filtered file. Are they real? If they have poor support, i.e., only 2-3 reads, go back to the VCF file and find novel entries with a reasonably high number in the `DP` column, then go back to IGV and inspect the position.



#### Is this useful?

Functional annotation of human genome variation is [a hard problem](http://massgenomics.org/2012/06/interpretation-of-human-genomes.html), but results from the last few years have been quite impressive. Dan Koboldt at WashU went through the 2011 literature and collected a large list of [disease-causing gene discoveries in disorders](http://massgenomics.org/2011/12/disease-causing-mutations-discovered-by-ngs-in-2011.html) and [cancer](http://massgenomics.org/2012/01/cancer-genome-and-exome-sequencing-in-2011.html), frequently with [relevance to the clinic](http://massgenomics.org/2010/04/why-we-sequence-cancer-genomes.html). A current manuscript studying Cystic Fibrosis shows that in some cases even [moderate samples sizes](http://www.ncbi.nlm.nih.gov/pubmed/22772370?dopt=Abstract) can be useful. Results are all but guaranteed, though -- for example, here are just the [most likely reasons why you cannot find a causative mutation in your data](http://massgenomics.org/2012/07/6-causes-of-elusive-mendelian-disease-genes.html).

> List recent paper that did >100 rare disease studies in parallel

In addition, sequencing errors and systematic changes to sample materials as part of the DNA extractation and library preparation process are a serious problem. Nick Lohman has a good summary of just [some of the known error sources](http://pathogenomics.bham.ac.uk/blog/2013/01/sequencing-data-i-want-the-truth-you-cant-handle-the-truth/) in a recent blog post triggered by a publication from the Broad Institute on [artifactual mutations due to oxidative damage](http://nar.oxfordjournals.org/content/early/2013/01/08/nar.gks1443.full.pdf?keytype=ref&ijkey=suYBLqdsrc7kH7G) -- in this case even analysis of the sample data with different sequencing technologies and bioinformatic workflows would not have made a difference. 




## Functional enrichment

> Tempted to scrap this whole part. Not really relevant for the course.

Additional tools can be used to re-prioritize SNPs when trying to make sense of data from a larger cohort. Most have been developed to gain additional information from GWA results by exploring variants beyond those reaching genome wide significance, but many can be adapted to combine candidate regions or genes from other data sources where the problem of finding relevant variants is compounded by inter-patient differences; i.e., the problem shifts from finding functional variants in single patients to finding consistency in a patient cohort, particularly for complex diseases.

A framework developed for GWAS data is [DAPPLE](http://www.broadinstitute.org/mpg/dapple/dapple.php) which connects variants to gene regions in LD before trying to find interacting genes and gene products to prioritize SNPs for validation. As a test, submit some [sample regions](https://dl.dropbox.com/u/407047/Blog/Prioritization/sampleRegions.txt) to the system.

Make sure to state that this is a `test run` with only 50 iterations, ask for a `plot` to be returned, and set `nearest gene` to false. We assume no prior knowledge and ask for links between all genes rather than just connections between query genes to seed genes. Once the system mails you your results take a look at the generated PDF and the prioritized genes. 
> What functional enrichment is detected in the generated network? What disease are you most likely dealing with?

Other systems such as [Grail](http://www.broadinstitute.org/mpg/grail/grail.php) use text mining approaches, that is, information obtained from analyzing PubMed abstracts, to link candidate genes based on co-occurring keywords. 

> Submit another [sample SNPs set](https://dl.dropbox.com/u/407047/Blog/Prioritization/sampleSNPs.txt) to the Grail server and select parameters as follows: `hg18`, `CEU population`, `Pubmed 2011`, `gene size correction is requested`, `query all genes`, `seed genes equal query genes`. Look at the report (this may take up to an hour) -- what disease are those variants most likely associated with?

Whether you used GRAIL or DAPPLE, extract the [the top genes](https://dl.dropbox.com/u/407047/Blog/Prioritization/dappleTopHits.txt) from the results (or use the link) and submit them to the [GeneMania](http://www.genemania.org/) system using default parameters to gain some additional insight.

Other systems can make use of case/control information, or sift through matched tumor/normal samples; [MuSiC](http://massgenomics.org/2012/07/mutation-significance-in-cancer.html), a recent release to analyze cancer mutation data sets also tests for pathway enrichment, known mutations and mutual exclusive mutations before linking findings to clinical covariates.






## A word on data management & reproducibility

* Should use version control, Software Carpentry
* For this course stick to basic management. See PLOS article. 
* Key to reproducibiity. Write everything down you do
* Always write the code, results for at least one collaborator: your future self 

**Resources:**

* http://swcarpentry.github.io/slideshows/best-practices/index.html#slide-0
* https://dl.dropboxusercontent.com/u/407047/Blog/Documents/Literature/Introduction/PLoS%20Comput%20Biol%202009%20Noble.pdf
* http://www.vox.com/2015/2/16/8034143/john-ioannidis-interview
* http://www.cdc.gov/genomics/public/features/science.htm
* http://software-carpentry.org/v5/novice/git/index.html

## Topics

* Validation, GiaB, DREAM
* https://github.com/chapmanb/bcbb/blob/master/posts/cancer_validation.org, https://github.com/chapmanb/bcbio-nextgen/blob/master/bcbio/variation/freebayes.py#L118 and https://github.com/chapmanb/bcbio-nextgen/blob/97060a79fe3dc82fb7cf7953495d52f0ba8c40ad/bcbio/variation/freebayes.py#L217

### Calling variants in a population

FreeBayes is designed to be run on many individuals from the same population (e.g. many human individuals) simultaneously. The algorithm exploits a neutral model of allele diffusion to impute most-confident genotypings across the entire population. In practice, the discriminant power of the method will improve if you run multiple samples simultaneously. In other words, if your study has multiple individuals, you should run freebayes against them at the same time. This also ensures consistent reporting of information about evidence for all samples at any locus where any are apparently polymorphic.

To call variants in a population of samples, each alignment must have a read group identifier attached to it (RG tag), and the header of the BAM file in which it resides must map the RG tags to sample names (SM). Furthermore, read group IDs must be unique across all the files used in the analysis. One read group cannot map to multiple samples. The reason this is required is that freebayes operates on a virtually merged BAM stream provided by the BamTools API. If merging the files in your analysis using bamtools merge would generate a file in which multiple samples map to the same RG, the files are not suitable for use in population calling, and they must be modified.

Users may add RG tags to BAM files which were generated without this information by using (as mentioned in "Calling variants" above) bamaddrg. If you have many files corresponding to many individuals, add a unique read group and sample name to each, and then open them all simultaneously with freebayes. The VCF output will have one column per sample in the input. 

## Resources 

 Feel free to work through all examples and explore the [reading material](http://scriptogr.am/ohofmann/reading). In addition to those links and manuscripts, SeqAnswers has a useful [short guide](http://seqanswers.com/wiki/How-to/exome_analysis#A_short_guide_to_Exome_sequencing_analysis_using_Illumina_technology) to Exome-seq analysis using the command line which helps when dealing with many samples.
